# NFS server on clusterd lvm + shared storage

#################################
# Scenario Requirements Section #
#################################
= REQUIREMENTS =
nodes=3
shared_storage=1
floating_ips=1
fence_cmd=1

packages=pacemaker corosync pcs dlm lvm2 lvm2-cluster nfs-utils resource-agents
cluster_init=1
clean_shared_storage=1

######################
# Deployment Scripts #
######################
= SCRIPTS =

##
# Initialize lvm
##
target=all
....
lvmconf --enable-cluster
systemctl disable nfs-server
systemctl stop nfs-server
systemctl disable nfs-lock
systemctl stop nfs-lock
....

##
# setup clvmd and dlm for clustered lvm management
##
target=$PHD_ENV_nodes1
....
tmpfile=$(mktemp tmpXXXX)
pcs resource defaults resource-stickiness=100
pcs cluster cib $tmpfile
pcs -f $tmpfile resource create dlm controld op monitor interval=30s on-fail=fence clone interleave=true ordered=true
pcs -f $tmpfile resource create clvmd clvm op monitor interval=30s on-fail=fence clone interleave=true ordered=true
pcs -f $tmpfile constraint order start dlm-clone then clvmd-clone
pcs -f $tmpfile constraint colocation add clvmd-clone with dlm-clone
pcs cluster cib-push $tmpfile

phd_wait_pidof "clvmd" 45
....

##
# Make the cluster storage volume group
##
target=$PHD_ENV_nodes1
....
dev=$PHD_ENV_shared_storage1

pvcreate -f $dev
vgcreate -cy cluster_vg $dev
echo "y" | lvcreate -L128M -n cluster_lv cluster_vg
mkfs.ext4 /dev/cluster_vg/cluster_lv
if [ $? -ne 0 ]; then
	echo "Failed to setup volume group"
	exit 1
fi
vgchange -an cluster_vg
....

##
# Make the lvm resources
##
target=$PHD_ENV_nodes1
....

pcs resource create cluster_vg LVM volgrpname=cluster_vg exclusive=true --group nfs-group
pcs resource create nfsshare Filesystem device=/dev/cluster_vg/cluster_lv directory=/nfsshare fstype=ext4 --group nfs-group
pcs constraint order start clvmd-clone then nfs-group
pcs colocation add nfs-group with clvmd-clone
# put the nfs group on a node we can predict,
# this just helps automate NFS share setup, but will be removed later.
pcs constraint location nfs-group prefers $PHD_ENV_nodes1

# For this scenario, we are using a 3rd node as a nfs client. We do not want
# the server to move to this node.
pcs constraint location nfs-group avoids $PHD_ENV_nodes3

# Wait for all resources to start
phd_rsc_verify_start_all 60

# Now that our mounts are up, setup the client folders
mkdir /nfsshare/clientdata
touch /nfsshare/clientdata/clientsharefile

# Remove that temporary location constraint now that the folders
# are initialized on the shared storage mount
pcs constraint location remove location-nfs-group-${PHD_ENV_nodes1}-INFINITY
....

##
# NFS share setup
##
target=$PHD_ENV_nodes1
....
suffix=$(echo "$PHD_ENV_floating_ips1" | awk -F. '{print $1 "." $2 "." $3 ".0"}')

pcs resource create nfs-daemon nfsserver nfs_shared_infodir=/nfsshare/nfsinfo --group nfs-group
pcs resource create nfs-export exportfs clientspec=${suffix}/255.255.255.0  options=rw,sync directory=/nfsshare/clientdata fsid=0 --group nfs-group
pcs resource create nfs-ip IPaddr2 ip=$PHD_ENV_floating_ips1 cidr_netmask=24 --group nfs-group

# Wait for all resources to start
phd_rsc_verify_start_all 60
....

##
# NFS client mount
##
target=$PHD_ENV_nodes3
....

systemctl start nfs-lock
# mount the share
pcs resource create nfs-client Filesystem device=${PHD_ENV_floating_ips1}:/nfsshare/clientdata  directory=/nfsclient fstype=nfs
pcs constraint location nfs-client prefers $PHD_ENV_nodes3

phd_rsc_verify_start_all 60

# verify we can establish locking
flock /nfsclient/clientsharefile -c "sleep 1"
if [ $? -ne 0 ]; then
	echo "Client failed to establish lock on nfs share file"
	exit 1
fi

....
