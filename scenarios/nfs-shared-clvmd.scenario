# NFS server on clusterd lvm + shared storage

#################################
# Scenario Requirements Section #
#################################
= REQUIREMENTS =
nodes=3
shared_storage=1
floating_ips=1
fence_cmd=1

packages=pacemaker corosync pcs dlm lvm2 lvm2-cluster nfs-utils resource-agents
cluster_init=1
clean_shared_storage=1

######################
# Deployment Scripts #
######################
= SCRIPTS =

##
# Initialize lvm
##
target=all
....
floater=$PHD_ENV_floating_ips1

lvmconf --enable-cluster
systemctl disable nfs-server
systemctl stop nfs-server

# Give the floating ip used for the NFS server
# a hostname 'floater'. This is required for sm-notify
# to work correctly during NFSv3 lock recovery. Clients
# use this hostname when connecting to the server
sed -i.bak "s/^...\....\....\..* floater.*$//p" /etc/hosts
echo "$floater floater" >> /etc/hosts

exit 0
....

##
# setup clvmd and dlm for clustered lvm management
##
target=$PHD_ENV_nodes1
....
tmpfile=$(mktemp tmpXXXX)
pcs resource defaults resource-stickiness=100
pcs cluster cib $tmpfile
pcs -f $tmpfile resource create dlm controld op monitor interval=30s on-fail=fence clone interleave=true ordered=true
pcs -f $tmpfile resource create clvmd clvm op monitor interval=30s on-fail=fence clone interleave=true ordered=true
pcs -f $tmpfile constraint order start dlm-clone then clvmd-clone
pcs -f $tmpfile constraint colocation add clvmd-clone with dlm-clone
pcs cluster cib-push $tmpfile

phd_wait_pidof "clvmd" 45
....

##
# Make the cluster storage volume group
##
target=$PHD_ENV_nodes1
....
dev=$PHD_ENV_shared_storage1

pvcreate -f $dev
vgcreate -cy cluster_vg $dev
echo "y" | lvcreate -L128M -n cluster_lv cluster_vg
mkfs.ext4 /dev/cluster_vg/cluster_lv
if [ $? -ne 0 ]; then
	echo "Failed to setup volume group"
	exit 1
fi


mkdir /nfsshare
mount /dev/cluster_vg/cluster_lv /nfsshare
mkdir -p /nfsshare/exports
mkdir -p /nfsshare/exports/export1
mkdir -p /nfsshare/exports/export2/
touch /nfsshare/exports/export1/clientsharefile1
touch /nfsshare/exports/export2/clientsharefile2
umount /nfsshare

vgchange -an cluster_vg
....

##
# Make the lvm resources
##
target=$PHD_ENV_nodes1
....

pcs resource create cluster_vg LVM volgrpname=cluster_vg exclusive=true --group nfs-group
pcs resource create nfsshare Filesystem device=/dev/cluster_vg/cluster_lv directory=/nfsshare fstype=ext4 --group nfs-group
pcs constraint order start clvmd-clone then nfs-group
pcs colocation add nfs-group with clvmd-clone

# Wait for all resources to start
phd_rsc_verify_start_all 60
....

##
# NFS share setup
##
target=$PHD_ENV_nodes1
....
suffix=$(echo "$PHD_ENV_floating_ips1" | awk -F. '{print $1 "." $2 "." $3 ".0"}')

pcs resource create nfs-daemon nfsserver nfs_no_notify=true nfs_shared_infodir=/nfsshare/nfsinfo --group nfs-group

# For this scenario, we are using a 3rd node as a nfs client.
# We do not want the server to move to this node.
pcs constraint location nfs-group avoids $PHD_ENV_nodes3

pcs resource create export-root exportfs clientspec=${suffix}/255.255.255.0  options=rw,sync,no_root_squash directory=/nfsshare/exports fsid=0 --group nfs-group
pcs resource create export-1 exportfs clientspec=${suffix}/255.255.255.0  options=rw,sync,no_root_squash directory=/nfsshare/exports/export1 fsid=1 --group nfs-group
pcs resource create export-2 exportfs clientspec=${suffix}/255.255.255.0  options=rw,sync,no_root_squash directory=/nfsshare/exports/export2 fsid=2 --group nfs-group
pcs resource create nfs-ip IPaddr2 ip=$PHD_ENV_floating_ips1 cidr_netmask=24 --group nfs-group
pcs resource create nfs-notify nfsnotify source_host=$PHD_ENV_floating_ips1 --group nfs-group

# Wait for all resources to start
phd_rsc_verify_start_all 60
....

##
# NFS v3 client mount
##
target=$PHD_ENV_nodes3
....

systemctl start nfs-lock

# mount the share
pcs resource create nfs-client-v3 Filesystem device=floater:/nfsshare/exports/export1  directory=/nfsclientv3 fstype=nfs
pcs constraint location nfs-client-v3 prefers $PHD_ENV_nodes3

phd_rsc_verify_start_all 60

# verify we can establish locking
if ! [ -f "/nfsclientv3/clientsharefile1" ]; then
	echo "Client failed to access client share file"
	exit 1
fi

flock /nfsclientv3/clientsharefile1 -c "sleep 1"
if [ $? -ne 0 ]; then
	echo "Client failed to establish lock on nfs share file"
	exit 1
fi

....

##
# NFS v4 client mount
##
target=$PHD_ENV_nodes3
....

# mount the share
pcs resource create nfs-client-v4 Filesystem device=floater:/  directory=/nfsclientv4 fstype=nfs4
pcs constraint location nfs-client-v4 prefers $PHD_ENV_nodes3

phd_rsc_verify_start_all 60
# verify we can establish locking
if ! [ -f "/nfsclientv4/export2/clientsharefile2" ]; then
	echo "Client failed to access client share file"
	exit 1
fi

flock /nfsclientv4/export2/clientsharefile2 -c "sleep 1"
if [ $? -ne 0 ]; then
	echo "Client failed to establish lock on nfs share file"
	exit 1
fi

....
