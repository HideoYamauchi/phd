# NFS server with multiple  nfsv4/nfsv3 exports

#################################
# Scenario Requirements Section #
#################################
= REQUIREMENTS =
nodes=4
shared_storage=6
floating_ips=2

packages=pacemaker corosync pcs nfs-utils resource-agents
cluster_init=1
clean_shared_storage=1

######################
# Deployment Scripts #
######################
= SCRIPTS =

##
# Initialize lvm
##
target=all
....
systemctl disable nfs-server
systemctl stop nfs-server
systemctl disable nfs-lock
systemctl stop nfs-lock

mkdir -p /mnt/nfs1
mkdir -p /mnt/nfs2
....

##
# Make the cluster storage partitions
##
target=$PHD_ENV_nodes1
....

ext4_create()
{
	dev=$1
	mkfs.ext4 $dev
	if [ $? -ne 0 ]; then
		echo "Failed to make ext4 fs on dev, $dev"
		exit 1
	fi

	# put a file with a consistent name on each filesystem so we can test locks
	mount $dev /mnt/nfs1
	echo "test client file" > /mnt/nfs1/clientdatafile
	umount /mnt/nfs1
}

ext4_create "$PHD_ENV_shared_storage1"
ext4_create "$PHD_ENV_shared_storage2"
ext4_create "$PHD_ENV_shared_storage3"
ext4_create "$PHD_ENV_shared_storage4"
ext4_create "$PHD_ENV_shared_storage5"
ext4_create "$PHD_ENV_shared_storage6"
....


##
# create nfs daemons
##
target=$PHD_ENV_nodes1
....

pcs resource create nfs-shared-info1 Filesystem device=${PHD_ENV_shared_storage5} directory=/mnt/nfs_sharedinfo_1 fstype=ext4 --group nfs-group1
pcs resource create nfs-shared-info2 Filesystem device=${PHD_ENV_shared_storage6} directory=/mnt/nfs_sharedinfo_2 fstype=ext4 --group nfs-group2

# put shares on separate nodes
pcs constraint colocation add nfs-group1 with nfs-group2 -INFINITY

pcs resource create nfs-daemon1 nfsserver nfs_shared_infodir=/mnt/nfs_sharedinfo_1 --group nfs-group1
pcs resource create nfs-daemon2 nfsserver nfs_shared_infodir=/mnt/nfs_sharedinfo_2 --group nfs-group2

# wait for all resources to start before moving on.
phd_rsc_verify_start_all 90
....

##
# create exports
##
target=$PHD_ENV_nodes1
....
suffix1=$(echo "$PHD_ENV_floating_ips1" | awk -F. '{print $1 "." $2 "." $3 ".0"}')
suffix2=$(echo "$PHD_ENV_floating_ips2" | awk -F. '{print $1 "." $2 "." $3 ".0"}')

# NOTE without setting wait_for_leasetime_on_stop=true, the exported fs will
# not be able to unmount if a nfsv4 client has an active file lease

#fsid 0 for both servers
pcs resource create nfs1-export-root exportfs clientspec=${suffix1}/255.255.255.0  options=rw,sync,no_root_squash directory=/mnt/nfs1 wait_for_leasetime_on_stop=true fsid=0 --group nfs-group1
pcs resource create nfs2-export-root exportfs clientspec=${suffix2}/255.255.255.0  options=rw,sync,no_root_squash directory=/mnt/nfs2 wait_for_leasetime_on_stop=true fsid=0 --group nfs-group2

# create fs mounts for shares
pcs resource create nfs1-fs1 Filesystem device=${PHD_ENV_shared_storage1} directory=/mnt/nfs1/export1 fstype=ext4 --group export1
pcs resource create nfs1-export1 exportfs clientspec=${suffix1}/255.255.255.0  options=rw,sync,no_root_squash directory=/mnt/nfs1/export1 wait_for_leasetime_on_stop=true fsid=1 --group export1

pcs resource create nfs1-fs2 Filesystem device=${PHD_ENV_shared_storage2} directory=/mnt/nfs1/export2 fstype=ext4 --group export2
pcs resource create nfs1-export2 exportfs clientspec=${suffix1}/255.255.255.0  options=rw,sync,no_root_squash directory=/mnt/nfs1/export2 wait_for_leasetime_on_stop=true fsid=2 --group export2

pcs resource create nfs2-fs1 Filesystem device=${PHD_ENV_shared_storage3} directory=/mnt/nfs2/export1 fstype=ext4 --group export3
pcs resource create nfs2-export1 exportfs clientspec=${suffix2}/255.255.255.0  options=rw,sync,no_root_squash directory=/mnt/nfs2/export1 wait_for_leasetime_on_stop=true fsid=3 --group export3

pcs resource create nfs2-fs2 Filesystem device=${PHD_ENV_shared_storage4} directory=/mnt/nfs2/export2 fstype=ext4 --group export4
pcs resource create nfs2-export2 exportfs clientspec=${suffix2}/255.255.255.0  options=rw,sync,no_root_squash directory=/mnt/nfs2/export2 wait_for_leasetime_on_stop=true fsid=4 --group export4

# server constraints
pcs constraint order start nfs-group1 then export1
pcs constraint order start nfs-group1 then export2
pcs constraint colocation add export1 with nfs-group1
pcs constraint colocation add export2 with nfs-group1

pcs constraint order start nfs-group2 then export3
pcs constraint order start nfs-group2 then export4
pcs constraint colocation add export3 with nfs-group2
pcs constraint colocation add export4 with nfs-group2

# wait for all resources to start before moving on.
phd_rsc_verify_start_all 90
....

##
# floating ips
##
target=$PHD_ENV_nodes1
....
pcs resource create vip1 IPaddr2 ip=$PHD_ENV_floating_ips1 cidr_netmask=24
pcs resource create vip2 IPaddr2 ip=$PHD_ENV_floating_ips2 cidr_netmask=24

pcs constraint colocation add vip1 with nfs-group1
pcs constraint colocation add vip2 with nfs-group2

pcs constraint order start export1 then vip1
pcs constraint order start export2 then vip1

pcs constraint order start export3 then vip2
pcs constraint order start export4 then vip2

# wait for all resources to start before moving on.
phd_rsc_verify_start_all 90
....

##
# client mounts
##
target=$PHD_ENV_nodes4
....
systemctl start nfs-lock

# keep the servers off this client node
pcs constraint location nfs-group1 avoids $PHD_ENV_nodes4
pcs constraint location nfs-group2 avoids $PHD_ENV_nodes4


tmpfile=$(mktemp tmpcibXXXXX)
pcs cluster cib $tmpfile

# mount the first nfs server as nfsv4
pcs -f $tmpfile resource create nfs-client-v4 Filesystem device=${PHD_ENV_floating_ips1}:/  directory=/nfsclientv4 fstype=nfs4

# mount the second one using nfsv3
pcs -f $tmpfile resource create nfs-client-v3 Filesystem device=${PHD_ENV_floating_ips2}:/mnt/nfs2/export1  directory=/nfsclientv3 fstype=nfs

pcs -f $tmpfile constraint location nfs-client-v3 prefers $PHD_ENV_nodes4
pcs -f $tmpfile constraint location nfs-client-v4 prefers $PHD_ENV_nodes4

pcs cluster cib-push $tmpfile
rm -f $tmpfile

# wait for all resources to start before moving on.
phd_rsc_verify_start_all 90
....

##
# sanity test client locking 
##
target=$PHD_ENV_nodes4
....

# verify we can establish locking
if ! [ -f "/nfsclientv3/clientdatafile" ]; then
	echo "Client failed to access client share file on nfsv3 mount"
	exit 1
fi

flock /nfsclientv3/clientdatafile -c "sleep 1"
if [ $? -ne 0 ]; then
	echo "Client failed to establish lock on nfsv3 share file"
	exit 1
fi

if ! [ -f "/nfsclientv4/export1/clientdatafile" ]; then
	echo "Client failed to access client share file on nfsv4 mount"
	exit 1
fi
flock /nfsclientv4/export1/clientdatafile -c "sleep 1"
if [ $? -ne 0 ]; then
	echo "Client failed to establish lock on nfsv4 share file"
	exit 1
fi

....
